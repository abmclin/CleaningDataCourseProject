###################################################################################################
#
# run_analysis
#
# Author:  Alexander McLin
#
# This script was written for the Johns Hopkins Cleaning Data course taken in September 2015
# on Coursera.org.
#
# Version 1.0
#
# Disclaimer:
#
# This script is provided without any warranty to the extent permitted by applicable law.
# You use it at your own risk.
#
# Purpose:
#
# Merge and tidy up the Human Activity Recognition Using Smartphones data set from UCI Machine
# Learning Repository located at <http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones>.
# A set of experiments performed using 30 volunteers using a Samsung Galaxy S II smartphone.
#
# Each person performed 6 activities, WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING,
# STANDING, LAYING. Each activity was measured (by the Samsung smartphone) at a rate of 50Hz.
#
# The signals were then sampled in fixed-width sliding windows resulting in 128 data points per window.
#
# The 128 data points were then futher processed into a feature vector of 561 measurements.
# One feature vector per window.
#
# The training and test datasets are comprised of the feature vectors derived from the windows
# generated by sampling the signals acquired during the experiments.
#
# More information about the experimental setup and data collection can be found at the UCI Machine
# Learning Repository URL.
#
# This R script merges all the training and test sets along with relevant labels into a single
# tidy dataset for futher analysis.
#
# Using the tidy dataset, the script will extract the mean and standard deviation measurements.
# Label each value with a descriptive name, subject and activity identifiers will also be included.
#
# Finally, the script will average all the extracted means and standard deviation values for each
# activity and subject and output the averages as a text file.
#
# Requirements:
#
# The experimental dataset must be available as a directory labeled UCI HAR Dataset in the same
# working directory run_analysis.R script is located in. You can download the dataset as a zip file
# via <https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip>
#
# Unzip the downloaded file and place the UCI HAR dataset directory into run_analysis.R's working
# directory.
#
# The dataset directory has a certain directory hierarchy which run_analysis.R is already familiar
# with.
#
# How to Run:
#
# You can run the script in two ways. Invoke <b>R</b> on the command-line then execute
# source("run_analysis.R") or invoke R --vanilla < run_analysis.R on the command-line.
#
# It will generate a text file called run_analysis_tidydataset which contains the subject,
# activity identifiers, and the averages of the measured mean and standard deviation values.
#
# Refer to the codebook.txt for details on the variables in the generated tidy dataset.
#
#################################################################################################


##################################################################################################
#
# Variables
#
##################################################################################################

# Basically capture the directory hierarchy in series of variables.

directoryPath = "./UCI HAR Dataset"
trainDirectoryPath = paste(directoryPath, "train", sep="/")
testDirectoryPath = paste(directoryPath, "test", sep="/")

# the labels files.
activity_labels_filePath = paste(directoryPath, "activity_labels.txt", sep="/")
features_filePath = paste(directoryPath, "features.txt", sep="/")

# the train and test dataset files.
X_train_filePath = paste(trainDirectoryPath, "X_train.txt", sep="/")
y_train_filePath = paste(trainDirectoryPath, "y_train.txt", sep="/")
subject_train_filePath = paste(trainDirectoryPath, "subject_train.txt", sep="/")

X_test_filePath = paste(testDirectoryPath, "X_test.txt", sep="/")
y_test_filePath = paste(testDirectoryPath, "y_test.txt", sep="/")
subject_test_filePath = paste(testDirectoryPath, "subject_test.txt", sep="/")

# Read in the actual datasets.
X_train = read.table(X_train_filePath)
y_train = read.table(y_train_filePath)
subject_train = read.table(subject_train_filePath)

X_test = read.table(X_test_filePath)
y_test = read.table(y_test_filePath)
subject_test = read.table(subject_test_filePath)

# Read in the activity and feature labels.
activity_labels = read.table(activity_labels_filePath)
features_labels = read.table(features_filePath)

##################################################################################################
#
# Main processing stage
#
##################################################################################################

# Use features labels to rename X_train and X_test columns. Note, use the second column in 
# features_labels for the names. We assume the order of the dataset is the same as the 
# features_labels vector.
names(X_train) <- features_labels[,2]
names(X_test) <- features_labels[,2]

# To make it easier to remember, we also rename the column names for y_train, y_test, 
# subject_train, subject_test, and activity_labels.
names(y_train) <- c("activity_id")
names(y_test) <- c("activity_id")
names(subject_train) <- c("subject")
names(subject_test) <- c("subject")
names(activity_labels) <- c("activity_id", "activity")

# We cbind the train and test dataset pieces together. We make the important assumption that all 
# the rows of the dataset are in same order with respect to each other.
combined_train = cbind(subject_train, y_train, X_train)
combined_test = cbind(subject_test, y_test, X_test)

# Then rbind the train and test datasets into a single dataset.
combined = rbind(combined_train, combined_test)

# merge together the combined dataset with activity_labels so we can associate the descriptive 
# names with the ids.
combined_activity_labeled = merge(activity_labels,combined, by.x="activity_id", by.y="activity_id")

# At this point, we need the dplyr package to select the colums we want from the 
# combined_activity_labeled dataset.

library(dplyr)

# Using dplyr select and contains to find the columns containing mean and std since we only want the
# mean and stand deviation values for each measurement. Also we want to keep the subject and 
# activity columns.
reduced_dataset = select(combined_activity_labeled, subject, activity, contains("mean"), contains("std"))

# Use group_by, summarize_each, and the mean functions to run analysis on the reduced_dataset. 
# Use piping semantics for better readability.
# The course project requires us to average the values for each activity and each subject. 
# So we group the reduced_dataset by activity and subject, then run mean over each grouped set 
# using summarize_each
run_analysis = (reduced_dataset %>% group_by(subject, activity) %>% summarize_each(funs(mean)))

# Need to rename the original labels derived from features_labels. They need to be more descriptive 
# since they have been encoded in a more terse format.
#
# Also the current labels pose problems for dplyr select since it cannot process label names 
# with illegal characters such as ()
# Futhermore, the values we are generating are actually averages of the measurements so the 
# features labels are no longer accurate.
#
# The renaming procedure I have chosen here:
# Prefix averageOf_ to all labels, except for subject and activity.
# Change dash to underscore.
# Change tBody and tGravity to Time_Body and Time_Gravity.
# Change fBody and fGravity to FrequencyDomain_Body and FrequencyDomain_Gravity.
# Rename parenthesis pairs to _function, so mean() and std() will become mean_function and 
# std_function.
renamedLabels = names(run_analysis[3:88]) # extract the labels from run_analysis for columnes 3 through 88
renamedLabels = gsub("-", "_", renamedLabels, fixed=TRUE)
renamedLabels = gsub("()", "_function", renamedLabels, fixed=TRUE)
renamedLabels = gsub("tBody", "Time_Body", renamedLabels, fixed=TRUE)
renamedLabels = gsub("tGravity", "Time_Gravity", renamedLabels, fixed=TRUE)
renamedLabels = gsub("fBody", "FrequencyDomain_Body", renamedLabels, fixed=TRUE)
renamedLabels = gsub("fGravity", "FrequencyDomain_Gravity", renamedLabels, fixed=TRUE)
renamedLabels = paste("averageOf_", renamedLabels, sep="")

# now rename the run_analysis columns based on the reformatted labels in renamedLabels.
names(run_analysis)[3:88] <- renamedLabels

# Write the run_analysis results to text file.
write.table(run_analysis,file="./run_analysis_tidydataset.txt", row.name=FALSE)
